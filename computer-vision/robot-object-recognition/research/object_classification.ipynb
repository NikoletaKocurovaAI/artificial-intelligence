{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ac7e29-1804-42ea-ad85-a5f222bb31d2",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a399f81-195b-417a-9303-c371a1090712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16d847bf-0b50-470c-9686-e6d71c23f48a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aadaba0b-53e2-4ece-a11a-89c4de530709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92eb3dd-f298-417c-b68b-04694c559624",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train CNN with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e343ad6e-3762-4f89-b82f-9c2673e94b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20d55357-095e-45fd-b0ac-5c8581f9563c",
   "metadata": {},
   "source": [
    "# Train CNN with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c8e408-32f3-4b4f-9d5d-175ece3152ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 18:35:35.507954: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.datasets import mnist\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c0c967a-49e5-444f-8a61-febbed735adf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "813f1d9f-c523-4e1c-95af-c344423a69d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89699d57-cb48-4af8-9411-4630d8cf2f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAJbElEQVR4nO3cX6jXdx3H8e85Hv9ks21mWxssZ+pSNpuVlDbRII7toosiTjJ2ZXTR1jZWBqsR9AeLFRHYsl0Mlhu0WmcU7aI/SIQMptZaLFY0YyqxaZYePCtn6X7n20286CLQ93eec34eH4/r34vP9+LA83xuPgNt27YNADRNMzjdHwBA/xAFAEIUAAhRACBEAYAQBQBCFAAIUQAghs71h8ODI5P5HQBMsl0To2f9jZsCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAxNN0fAGczMFT/M531pkWT8CXnx/OfubbTrjd/orxZvPRv5c382wfKm79+c05588yax8qbpmmaY72T5c17RreWN8s+vbe8mQncFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQDCg3gzzKyVy8ubdu7s8ubwxsvKm1Nr6w+ZNU3TLLy0vnvyxm6Prc00P3tlQXnztW/fXN7sW/VoeXPwzKnypmma5r6jw+XN1U+2nc66GLkpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAMRA27bn9FLU8ODIZH8L/6P3vnd22m3fuaO8uW72nE5nMbXOtL3y5r1fv7u8GTo5NY/HLXjp1U67ucfqD+m1Tz/X6ayZZtfE6Fl/46YAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQAxN9wfw/819/nCn3W//dU15c93so53Ommm2Hllb3hz456LyZufSx8ubpmma8Yn666VXfuupTmf1s6l5w/Xi5aYAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEANt257T+1LDgyOT/S2cB2Nb1pU3L998sryZ9ftLyptnb7+/vOlq27G3lze/2Vh/3K53Yry8adfdWN40TdMcuqu+WXLLs53OYmbaNTF61t+4KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEB/FoZi16Y3nTOz5W3hx8tP5IXdM0zR82PFTevPurd5Y3V+x4qryBC4kH8QAoEQUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAghqb7A5h+vWPHp+ScMy/PmZJzmqZprr/1j+XN3x+YVT9oolffQB9zUwAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgvJLKlFl5z/5Ouy2r3l/efHfxL8ubjSOfLG8WPLa3vIF+5qYAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEB7EY8r0Tox32h2/bWV585cnTpU3n932SHnzuY9+uLxpf3dpedM0TXPNV/bUR23b6SwuXm4KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCADHQtuf2Ytbw4MhkfwucN2MfW1fefO8L3yhvlgzNK2+6uv6RO8qb5Q8eKW9ePXCovOHCsGti9Ky/cVMAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACA/iwX+1N60ub95w34vlzfff+ovypqsVv/p4efO2L42XN70/HyhvmHoexAOgRBQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGA8CAevAazrryivDm8eVmns/bds728Gezwf9+tBzeVN+Prj5c3TD0P4gFQIgoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIA4ZVUuED88MU95c38gTnlzSvt6fLmg3feXd7M//G+8obXxiupAJSIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABBD0/0B0C8m1q8ub14YmVfe3LD6UHnTNN0et+vi/rF3lDfzf/L0JHwJ08FNAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACA8iEffG1hzQ3mz/67643EP3vRwebNh3unyZir9uz1T3uwdW1I/aOJIfUNfclMAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACA/i0cnQksXlzQtbru501hc3/6C8+cglxzqd1c/uPbqmvNm9fW15c/nDe8obZg43BQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYDwIN4MM3TtW8qb8XddVd5s/vLPy5tPXPaj8qbfbT1Sf3Buz3fqD9s1TdMs3Pnr8ubyCY/bUeOmAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEB4JXUKDF315vJm7KHXdzrrtiW7y5tbFhztdFY/u+Ol9eXNMw+sLm8WPf5cebPwH14upX+5KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDERf0g3ukPrKlvPjVW3ty77KflzabXnSxv+t3R3qlOuw1PbC1vVnz+T+XNwhP1h+omygvob24KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCAHFRP4h36EP1Ju5fNToJX3L+7DixtLzZvntTeTPQGyhvVmw7WN40TdMsP7qvvOl1OglwUwAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFACIgbZt23P54fDgyGR/CwCTaNfE2R/0dFMAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAGKgbdt2uj8CgP7gpgBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAED8BwdNKpY4Umj7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(train_images[0])\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a105c5ed-38ac-40de-9d3f-9aae1b92a827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5a604e6-2010-41d4-a861-edf24425f5c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "316fe50a-f415-4bd5-b9f0-e832db9e444d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8646cd5a-2290-4ddf-9656-751eabdeec92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8d47139-c853-4849-b23b-41485fb8bcc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cff2ce-bd12-48bf-abad-2ff11c9f272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# resape and scale\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "\n",
    "# Since pixel values in typical images are in the range of 0 to 255 (for 8-bit images), dividing by 255 scales the pixel values to \n",
    "# the range [0, 1]. This is known as normalization.\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2d1db10-a11c-4f0e-9b9d-1f6d2cb927ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# The function to_categorical is typically used when dealing with classification tasks\n",
    "# It converts class labels into one-hot encoded vectors\n",
    "\n",
    "# In classification tasks, especially with neural networks, the output layer usually has neurons corresponding to each class. \n",
    "# For example, if you have 10 classes, you would have 10 output neurons, each representing the probability of belonging to a particular class.\n",
    "\n",
    "# One-hot encoding is a binary representation of categorical variables where each class is represented by a binary vector. \n",
    "# In this representation, only one bit is high (1) while all others are low (0). This helps the network to understand the categorical \n",
    "# nature of the output and interpret it correctly during training.\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "155ae006-378b-45bf-adff-86a9ed039cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25decab0-6aa0-4754-a0ee-190629303fe3",
   "metadata": {},
   "source": [
    "## Architecture examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0079a8e7-ad37-4284-9727-c32d28853391",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### simple feedforward neural network with fully connected layers ###\n",
    "\n",
    "# Feedforward neural networks are typically used for tasks where the input data is represented as a flat vector, such as tabular data or \n",
    "# pre-extracted features. In the context of image data, the approach of flattening the image into a vector and feeding it directly into a \n",
    "# feedforward neural network is often less effective compared to using convolutional neural networks (CNNs).\n",
    "\n",
    "# network consists of sequence of 2 Dense layers, which are densely/fully connected\n",
    "network = models.Sequential()\n",
    "\n",
    "# (28 * 28,) corresponds to a 1D shape of length 784. This means the input data is expected to be a flat vector of length 784 \n",
    "# (such as flattened images with dimensions 28x28).\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "\n",
    "# softmax = array of 10 probability scores\n",
    "network.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb44dfb-507f-4f1b-9004-d05122c2555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "# This layer will return a tensor where the first dimension has been trans- formed to be 32.\n",
    "# the default activation function is the linear activation function\n",
    "# Here, input_shape=(784,) specifies the shape of the input data expected by the first layer of the model. \n",
    "# Similarly, (784,) corresponds to a 1D shape of length 784. This also means the input data is expected to be a flat vector of length 784.\n",
    "model.add(layers.Dense(32, input_shape=(784,)))\n",
    "\n",
    "# Thus this layer can only be connected to a downstream layer that expects 32- dimensional vectors as its input.\n",
    "# # the default activation function is the linear activation function\n",
    "model.add(layers.Dense(32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e76fed1-4fb1-4a11-b066-d5546728f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(784,)))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# And here’s the same model defined using the functional API:\n",
    "\n",
    "input_tensor = layers.Input(shape=(784,))\n",
    "x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
    "model = models.Model(inputs=input_tensor, outputs=output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceda39f-b5c3-4247-96f2-89d40dea4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "activation='relu', input_shape=(10000,))) model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                       activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# l1\n",
    "# regularizers.l1(0.001)\n",
    "\n",
    "# simultaneous\n",
    "# regularizers.l1_l2(l1=0.001, l2=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce1ad83-66e6-4b9f-951f-dda593abfa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### a feedforward neural network with fully connected layers, but with the addition of dropout regularization. ###\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5736b2-4a6c-4c84-8601-d213e37afc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a dot product (dot) between the input tensor and a tensor named W\n",
    "# an addition (+) between the resulting 2D ten- sor and a vector b\n",
    "# a relu operation. relu(x) is max(x, 0)\n",
    "output = relu(dot(W, input) + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56943aa9-1871-47e6-b339-ec29d2d4d402",
   "metadata": {},
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af92b1c-6257-46ca-8772-24b6a21e44c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96efcf31-7868-4a40-8be3-410834e339ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b36801-416b-48e2-a733-979f1e3d1bb7",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117ba6b3-b683-4f4d-a5c2-11a2dff15b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ccc7c-cbf8-4b13-9c3f-09a4eea15d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit(input_tensor, target_tensor, batch_size=128, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621cf00b-bb73-4640-86bd-e3c74e4ef578",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10cd53-397c-402f-b94f-6770e74b5b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2fb5b-26f5-4c0f-b997-c20592a2db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CONVNETS\n",
    "\n",
    "from keras import layers \n",
    "from keras import models\n",
    "\n",
    "# a convolutional neural network (CNN), which is specifically designed to work with image data. It's more suitable for tasks where the \n",
    "# input data is structured in multiple dimensions, such as images represented as grids of pixels. \n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "# Conv2D\n",
    "# a fundamental building block used for feature extraction from input images. \n",
    "# Convolution Operation: At its core, the Conv2D layer performs a convolution operation between the input image and a set of learnable filters \n",
    "# (also known as kernels or convolutional kernels). Each filter is a small matrix of weights.\n",
    "# Feature Map: The convolution operation is applied by sliding each filter over the input image and computing the dot product between the \n",
    "# filter and the input patch at each position. This produces a feature map, which is a 2D representation of the activations of that filter \n",
    "# across the input image.\n",
    "# Multiple Filters: Typically, a Conv2D layer has multiple filters, and each filter learns to detect a different pattern or feature in the \n",
    "# input image. For example, early filters might learn to detect edges or corners, while deeper filters might learn more complex patterns.\n",
    "# Parameters: The weights of the filters in the Conv2D layer are the parameters that are learned during the training process. The goal of \n",
    "# training is to learn filters that can extract meaningful features from the input images for the task at hand (e.g., classification, \n",
    "# object detection).\n",
    "# Padding: Optionally, the Conv2D layer can apply padding to the input image before performing the convolution operation. Padding adds extra \n",
    "# rows and columns of zeros around the input image, which helps preserve spatial dimensions and prevent information loss at the edges of \n",
    "# the image.\n",
    "# Strides: Additionally, the Conv2D layer can use a stride parameter to specify the step size of the filter as it slides over the input image. \n",
    "# Larger stride values result in downsampling of the feature map, reducing its spatial dimensions.\n",
    "\n",
    "# 32: This number represents the number of filters (also known as kernels or convolutional channels) in the convolutional layer. \n",
    "# Each filter is a small matrix that is applied to the input image to extract features. Having multiple filters allows the network to \n",
    "# learn different features at each location in the input image. In this case, there are 32 filters in the convolutional layer.\n",
    "# (3, 3): This tuple represents the size of each filter in the convolutional layer. Each filter is a small matrix with dimensions specified \n",
    "# by this tuple. \n",
    "# In this case, each filter is a 3x3 matrix.\n",
    "\n",
    "# Conv2D(output_depth, (window_height, window_width))\n",
    "\n",
    "# input_shape=(image_height, image_width, image_channels)\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) \n",
    "\n",
    "# MaxPooling2D\n",
    "# a convolutional neural network (CNN) is to downsample the input representation, reducing its spatial dimensions. This helps in reducing the \n",
    "# computational complexity of the network, as well as providing a form of translation invariance.\n",
    "# Here's how the MaxPooling2D layer works:\n",
    "# It operates on each feature map independently.\n",
    "# For each region of the input feature map specified by the pool size (in this case, (2, 2)), the maximum value within that region is retained.\n",
    "# The output of the operation is a downsampled version of the input feature map, where the spatial dimensions are reduced by a factor \n",
    "# determined by the pool size.\n",
    "# For example, if you have a (6, 6) input feature map and apply MaxPooling2D((2, 2)), you'll get a (3, 3) output feature map, as each (2, 2) \n",
    "# region is reduced to a single value (the maximum value in that region).\n",
    "# The purpose of this downsampling operation is to:\n",
    "# Reduce the number of parameters in the network, thus reducing computational complexity and memory requirements.\n",
    "# Increase the receptive field of later layers, allowing them to capture more abstract features by aggregating information from larger \n",
    "# regions of the input.\n",
    "# Provide a form of translation invariance, making the network more robust to small variations in the position of features within the input \n",
    "# images.\n",
    "\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu')) \n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# You can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). \n",
    "# The width and height dimensions tend to shrink as you go deeper in the network. \n",
    "# The number of channels is controlled by the first argument passed to the Conv2D layers (32 or 64).\n",
    "model.summary()\n",
    "\n",
    "# The next step is to feed the last output tensor (of shape (3, 3, 64)) into a densely connected classifier network like those you’re \n",
    "# already familiar with: a stack of Dense layers. These classifiers process vectors, which are 1D, whereas the current output is a \n",
    "# 3D tensor. First we have to flatten the 3D outputs to 1D, and then add a few Dense lay- ers on top.\n",
    "# We’ll do 10-way classification, using a final layer with 10 outputs and a softmax activa- tion\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# the (3, 3, 64) outputs are flattened into vectors of shape (576,) before going through two Dense layers.\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a362f445-a84a-4c39-9ded-5c4d6625837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The fundamental difference between a densely connected layer and a convolution layer is this: Dense layers learn global patterns in their \n",
    "# input feature space (for exam- ple, for a MNIST digit, patterns involving all pixels), whereas convolution layers learn local patterns \n",
    "# (see figure 5.1): in the case of images, patterns found in small 2D win- dows of the inputs. In the previous example, these windows were \n",
    "# all 3 × 3.\n",
    "\n",
    "# This key characteristic gives convnets two interesting properties:\n",
    "# The patterns they learn are translation invariant. After learning a certain pattern in the lower-right corner of a picture, a convnet \n",
    "# can recognize it anywhere: for example, in the upper-left corner. A densely connected network would have to learn the pattern anew \n",
    "# if it appeared at a new location. This makes convnets data efficient when processing images (because the visual world is fundamentally \n",
    "# translation invariant): they need fewer training samples to learn representations that have generalization power.\n",
    "# They can learn spatial hierarchies of patterns (see figure 5.2). A first convolution layer will learn small local patterns such as edges, \n",
    "# a second convolution layer will learn larger patterns made of the features of the first layers, and so on. This allows convnets to efficiently \n",
    "# learn increasingly complex and abstract visual con- cepts (because the visual world is fundamentally spatially hierarchical).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
